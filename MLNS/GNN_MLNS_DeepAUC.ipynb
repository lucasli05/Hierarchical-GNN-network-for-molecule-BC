{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"17Lhl5YMPpmTTvmIHOTOr-9tJqlqzIiPq","timestamp":1745585494081},{"file_id":"1ooOo1Lt9rLRMBuSNHxzkN9XhoZvC44I3","timestamp":1745567695132},{"file_id":"1PvF2F4a3tJ8wn0_Ub80nJF-6Y8arxvC7","timestamp":1745564336164},{"file_id":"1JFVUoYfyTIrLSrMni75EuzeiSemDb3RR","timestamp":1745550745736},{"file_id":"13LRGnul8JNyrSazM6LuRbrddgxvPNqls","timestamp":1745518870888},{"file_id":"1JOONx0FdUkl21EuBb548q_cWdIgMMISh","timestamp":1745511696147}],"gpuType":"A100","machine_shape":"hm","authorship_tag":"ABX9TyNXNImWwt0KU/2UVdf2EfPk"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aow-FLJMeEjE","executionInfo":{"status":"ok","timestamp":1745586015647,"user_tz":-120,"elapsed":29,"user":{"displayName":"Thomas G","userId":"10343934438851165110"}},"outputId":"39ab8352-6abf-4c73-bd43-227b55eb7468","collapsed":true},"outputs":[{"output_type":"stream","name":"stdout","text":["2.6.0+cu124 12.4\n"]}],"source":["import torch\n","print(torch.__version__, torch.version.cuda)"]},{"cell_type":"code","source":["!pip install --quiet \\\n","  torch-scatter     -f https://data.pyg.org/whl/torch-2.6.0+cu124.html \\\n","  torch-sparse      -f https://data.pyg.org/whl/torch-2.6.0+cu124.html \\\n","  torch-cluster     -f https://data.pyg.org/whl/torch-2.6.0+cu124.html \\\n","  torch-spline-conv -f https://data.pyg.org/whl/torch-2.6.0+cu124.html \\\n","  torch-geometric\n","\n","# OGB, RDKit, LibAUC, and helpers\n","!pip install --quiet ogb rdkit libauc tqdm scikit-learn\n","!git clone https://github.com/lightaime/deep_gcns_torch.git /content/deep_gcns_torch"],"metadata":{"id":"7QuzAJzS9PW_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1745586020735,"user_tz":-120,"elapsed":5079,"user":{"displayName":"Thomas G","userId":"10343934438851165110"}},"outputId":"100cbce1-2978-46cb-cc13-fcde30e73f88"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["fatal: destination path '/content/deep_gcns_torch' already exists and is not an empty directory.\n"]}]},{"cell_type":"code","source":["import os,sys, time, pathlib, logging, random\n","import numpy as np, pandas as pd\n","import copy\n","import torch_geometric\n","import torch.nn.functional as F\n","from torch_geometric.nn import global_add_pool, global_mean_pool, global_max_pool\n","from tqdm.auto import tqdm\n","from rdkit import Chem,RDLogger\n","from rdkit.Chem import AllChem\n","from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import roc_auc_score\n","from torch_geometric.data import DataLoader\n","from ogb.graphproppred import PygGraphPropPredDataset, Evaluator\n","from torch.serialization import safe_globals\n","from torch_geometric.data.data import DataEdgeAttr\n","from libauc.losses import AUCMLoss\n","from libauc.optimizers import PESG\n","from google.colab import drive\n","from rdkit.Chem import rdMolDescriptors, MACCSkeys\n","sys.path.append(\"/content/deep_gcns_torch\")\n","RDLogger.DisableLog('rdApp.*')\n","drive.mount('/content/drive', force_remount=True)\n","project_root = pathlib.Path('/content/drive/MyDrive/MLNS')\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","from ogb.graphproppred.mol_encoder import AtomEncoder, BondEncoder\n","from gcn_lib.sparse.torch_vertex import GENConv\n","from gcn_lib.sparse.torch_nn import norm_layer, MLP\n","\n","# mimic ArgsInit\n","class CFG:\n","    dataset     = \"ogbg-molhiv\"\n","    batch_size  = 256\n","    lr          = 0.01\n","    epochs_pre  = 300\n","    epochs_ft   = 100\n","    num_workers = 4\n","    num_tasks = 1\n","    random_seed = 0\n","    hidden_channels  = 256\n","    num_layers      = 14\n","    dropout     = 0.5\n","    block       = \"res+\"\n","    gcn_aggr    = \"softmax\"\n","    t, p, y     = 1.0, 1.0, 0.0\n","    learn_t     = True\n","    learn_p     = False\n","    learn_y     = False\n","    msg_norm    = False\n","    learn_msg   = False\n","    learn_msg_scale = False\n","    conv_encode_edge = False\n","    add_virtual_node = False\n","    conv = 'gen'\n","    optimizer   = \"pesg\"\n","    gamma       = 500\n","    margin      = 1.0\n","    weight_decay= 1e-4\n","    activations  = \"relu\"  # or \"elu\"\n","    norm         = \"batch\" # batchnorm in GENConv\n","    mlp_layers   = 1       # MLP depth in each GENConv\n","    graph_pooling = \"mean\"  # could be \"sum\" or \"max\"\n","\n","cfg = CFG()\n","torch.manual_seed(cfg.random_seed)\n","np.random.seed(cfg.random_seed)"],"metadata":{"id":"cjBZNDC-96cA","executionInfo":{"status":"ok","timestamp":1745586032334,"user_tz":-120,"elapsed":11597,"user":{"displayName":"Thomas G","userId":"10343934438851165110"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"d8928dbf-4de6-42f0-af0d-859b0a8e08e0"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import torch\n","_torch_load = torch.load\n","\n","def _torch_load_override(f, *args, **kwargs):\n","    # if weights_only wasn't explicitly set, force it to False\n","    if \"weights_only\" not in kwargs:\n","        kwargs[\"weights_only\"] = False\n","    return _torch_load(f, *args, **kwargs)\n","\n","# override torch.load globally\n","torch.load = _torch_load_override\n","from torch.serialization import add_safe_globals\n","\n","# import the class that needs to be allow-listed\n","from torch_geometric.data.data import DataEdgeAttr\n","\n","# allow it for all future torch.load calls\n","add_safe_globals([DataEdgeAttr])\n","\n","# now this will succeed\n","from ogb.graphproppred import PygGraphPropPredDataset\n","dataset = PygGraphPropPredDataset(name=cfg.dataset)\n","all_labels = dataset.data.y.view(-1).numpy()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HLNZ7ikhImiv","executionInfo":{"status":"ok","timestamp":1745586039954,"user_tz":-120,"elapsed":7616,"user":{"displayName":"Thomas G","userId":"10343934438851165110"}},"outputId":"df2c1d52-6355-4c0d-9581-34ef474d960a"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading http://snap.stanford.edu/ogb/data/graphproppred/csv_mol_download/hiv.zip\n"]},{"output_type":"stream","name":"stderr","text":["Downloaded 0.00 GB: 100%|██████████| 3/3 [00:02<00:00,  1.12it/s]\n","Processing...\n"]},{"output_type":"stream","name":"stdout","text":["Extracting dataset/hiv.zip\n","Loading necessary files...\n","This might take a while.\n","Processing graphs...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 41127/41127 [00:00<00:00, 112667.29it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Converting graphs into PyG objects...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 41127/41127 [00:01<00:00, 30292.65it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Saving...\n"]},{"output_type":"stream","name":"stderr","text":["Done!\n","/usr/local/lib/python3.11/dist-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n","  warnings.warn(msg)\n"]}]},{"cell_type":"code","source":["project_root = pathlib.Path('/content/drive/MyDrive/MLNS')\n","df = pd.read_csv(project_root / 'HIV.csv')\n","smiles = df['smiles'].tolist()\n","labels = df['activity'].map({'CI': 0, 'CM': 1}).tolist()"],"metadata":{"id":"fJbfAT5e-lus","executionInfo":{"status":"ok","timestamp":1745586045362,"user_tz":-120,"elapsed":5404,"user":{"displayName":"Thomas G","userId":"10343934438851165110"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["saved_data = copy.deepcopy(dataset._data)   # or dataset.data\n","\n","# # … now you can safely monkey-patch torch.load or whatever without losing this …\n","\n","# # 2) Whenever you want to re-attach the RF column:\n","# dataset._data = copy.deepcopy(saved_data)   # restore the original\n","# orig_y = dataset._data.y.view(-1,1)         # (N,1) true labels\n","# all_labels = dataset.data.y.view(-1).numpy()"],"metadata":{"id":"cZTiJ61xxVk7","executionInfo":{"status":"ok","timestamp":1745586045395,"user_tz":-120,"elapsed":11,"user":{"displayName":"Thomas G","userId":"10343934438851165110"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"viRmOizt5lnJ","executionInfo":{"status":"ok","timestamp":1745586045424,"user_tz":-120,"elapsed":12,"user":{"displayName":"Thomas G","userId":"10343934438851165110"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["from rdkit import Chem\n","from rdkit.Chem import rdMolDescriptors, MACCSkeys\n","\n","def fp_vect(smi):\n","    \"\"\"\n","    Compute Morgan and MACCS fingerprints for a SMILES string using\n","    RDKit's rdMolDescriptors.GetMorganFingerprintAsBitVect API.\n","    Returns:\n","      - morgan: numpy array of shape (2048,) with 0/1 entries\n","      - maccs: numpy array of shape (166,) with 0/1 entries\n","    If SMILES fails (parsing or sanitization), returns (None, None).\n","    \"\"\"\n","    mol = Chem.MolFromSmiles(smi)\n","    if mol is None:\n","        return None, None\n","\n","    # attempt sanitization (catches valence errors, etc.)\n","    try:\n","        Chem.SanitizeMol(mol)\n","    except Exception:\n","        return None, None\n","\n","    # Morgan fingerprint (radius=2, 2048 bits, no chirality)\n","    morgan_bv = rdMolDescriptors.GetMorganFingerprintAsBitVect(\n","        mol, radius=2, nBits=2048, useChirality=False\n","    )\n","    morgan = np.array(morgan_bv, dtype=int)\n","\n","    # MACCS keys (166-bit)\n","    maccs_bv = MACCSkeys.GenMACCSKeys(mol)\n","    maccs = np.array(maccs_bv, dtype=int)\n","\n","    return morgan, maccs"],"metadata":{"id":"9aobmxSBDQiP","executionInfo":{"status":"ok","timestamp":1745586045459,"user_tz":-120,"elapsed":13,"user":{"displayName":"Thomas G","userId":"10343934438851165110"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["valid_entry_idxs = []\n","for i, (smi, y) in enumerate(zip(smiles, all_labels)):\n","    # skip missing labels\n","    if np.isnan(y):\n","        continue\n","    mg, mc = fp_vect(smi)\n","    # skip bad SMILES\n","    if mg is None:\n","        continue\n","    valid_entry_idxs.append(i)\n","valid_set = set(valid_entry_idxs)"],"metadata":{"id":"2usCdP_lyJQP","executionInfo":{"status":"ok","timestamp":1745586151037,"user_tz":-120,"elapsed":105555,"user":{"displayName":"Thomas G","userId":"10343934438851165110"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["split_idx = dataset.get_idx_split()\n","orig_train = split_idx[\"train\"]  # numpy array of ints\n","orig_val   = split_idx[\"valid\"]\n","orig_test  = split_idx[\"test\"]\n","\n","train_filt = np.intersect1d(orig_train, valid_entry_idxs).tolist()\n","val_filt   = np.intersect1d(orig_val,   valid_entry_idxs).tolist()\n","test_filt  = np.intersect1d(orig_test,  valid_entry_idxs).tolist()\n","\n","print(len(orig_train), \"→\", len(train_filt),\n","      len(orig_val),   \"→\", len(val_filt),\n","      len(orig_test),  \"→\", len(test_filt))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CZmJjSwhy1Ew","executionInfo":{"status":"ok","timestamp":1745586151085,"user_tz":-120,"elapsed":38,"user":{"displayName":"Thomas G","userId":"10343934438851165110"}},"outputId":"b9f248a3-0c9a-4ce5-f874-92a77a289fd7"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["32901 → 32898 4113 → 4111 4113 → 4111\n"]}]},{"cell_type":"code","source":["morgan_feats, maccs_feats, labels_tv = [], [], []\n","for i in valid_entry_idxs:    # all valid entries\n","    mg, mc = fp_vect(smiles[i])\n","    morgan_feats.append(mg)\n","    maccs_feats.append(mc)\n","    labels_tv.append(int(all_labels[i]))\n","\n","X_all = np.concatenate([np.stack(morgan_feats), np.stack(maccs_feats)], axis=1)\n","y_all = np.array(labels_tv, dtype=int)\n","\n","# Get local positions of each split\n","idx_map   = {orig:loc for loc, orig in enumerate(valid_entry_idxs)}\n","train_loc = [idx_map[i] for i in train_filt]\n","val_loc   = [idx_map[i] for i in val_filt]\n","test_loc  = [idx_map[i] for i in test_filt]\n","\n","X_train, y_train = X_all[train_loc], y_all[train_loc]\n","X_val,   y_val   = X_all[val_loc],   y_all[val_loc]\n","X_test,  y_test  = X_all[test_loc],  y_all[test_loc]\n"],"metadata":{"id":"5WrmUtMYzD5U","executionInfo":{"status":"ok","timestamp":1745586261541,"user_tz":-120,"elapsed":110454,"user":{"displayName":"Thomas G","userId":"10343934438851165110"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["rf = RandomForestClassifier(\n","    n_estimators=1000, class_weight={0:1,1:10}, random_state=cfg.random_seed, n_jobs=-1)\n","rf.fit(X_train, y_train)\n","probs_all = rf.predict_proba(X_all)[:,1]"],"metadata":{"id":"01Ms7ObgFDt1","executionInfo":{"status":"ok","timestamp":1745586322087,"user_tz":-120,"elapsed":59998,"user":{"displayName":"Thomas G","userId":"10343934438851165110"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["split_idx   = dataset.get_idx_split()\n","train_idx   = split_idx[\"train\"]\n","valid_idx   = split_idx[\"valid\"]\n","test_idx    = split_idx[\"test\"]\n","\n","loaders = {\n","    \"train\": DataLoader(dataset[train_idx], batch_size=cfg.batch_size, shuffle=True,  num_workers=cfg.num_workers),\n","    \"valid\": DataLoader(dataset[valid_idx], batch_size=cfg.batch_size, shuffle=False, num_workers=cfg.num_workers),\n","    \"test\":  DataLoader(dataset[test_idx],  batch_size=cfg.batch_size, shuffle=False, num_workers=cfg.num_workers),\n","}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UBR5k3Utqy4s","executionInfo":{"status":"ok","timestamp":1745586322146,"user_tz":-120,"elapsed":46,"user":{"displayName":"Thomas G","userId":"10343934438851165110"}},"outputId":"8c0f67c8-a6b3-4429-8587-530d6cf551d3"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n","  warnings.warn(out)\n"]}]},{"cell_type":"code","source":["N = len(dataset)    # total number of graphs in the full OGB split\n","rf_full = np.zeros(N, dtype=float)\n","for local_pos, orig_idx in enumerate(valid_entry_idxs):\n","    rf_full[orig_idx] = probs_all[local_pos]\n","\n","orig_y = dataset.data.y.view(-1,1).clone()           # (N,1) true labels\n","rf_col = torch.from_numpy(rf_full).view(-1,1).float()  # (N,1) RF pos-probs\n","\n","dataset.data.y = torch.cat([orig_y, rf_col], dim=1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7hgpotlB6LA4","executionInfo":{"status":"ok","timestamp":1745586322202,"user_tz":-120,"elapsed":52,"user":{"displayName":"Thomas G","userId":"10343934438851165110"}},"outputId":"c61479ed-90e9-4aac-b934-32a2992cd39c"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n","  warnings.warn(msg)\n"]}]},{"cell_type":"code","source":["# ---- DeeperGCN.forward ----\n","\n","class DeeperGCN(torch.nn.Module):\n","    def __init__(self, args):\n","        super(DeeperGCN, self).__init__()\n","\n","        self.num_layers = args.num_layers\n","        self.dropout = args.dropout\n","        self.block = args.block\n","        self.conv_encode_edge = args.conv_encode_edge\n","        self.add_virtual_node = args.add_virtual_node\n","\n","        hidden_channels = args.hidden_channels\n","        num_tasks = args.num_tasks\n","        conv = args.conv\n","        aggr = args.gcn_aggr\n","        t = args.t\n","        self.learn_t = args.learn_t\n","        p = args.p\n","        self.learn_p = args.learn_p\n","        y = args.y\n","        self.learn_y = args.learn_y\n","\n","        self.msg_norm = args.msg_norm\n","        learn_msg_scale = args.learn_msg_scale\n","        self.activation_func = F.relu if args.activations=='relu' else F.elu\n","\n","        norm = args.norm\n","        mlp_layers = args.mlp_layers\n","\n","        graph_pooling = args.graph_pooling\n","\n","        print('The number of layers {}'.format(self.num_layers),\n","              'Aggr aggregation method {}'.format(aggr),\n","              'block: {}'.format(self.block))\n","        if self.block == 'res+':\n","            print('LN/BN->ReLU->GraphConv->Res')\n","        elif self.block == 'res':\n","            print('GraphConv->LN/BN->ReLU->Res')\n","        elif self.block == 'dense':\n","            raise NotImplementedError('To be implemented')\n","        elif self.block == \"plain\":\n","            print('GraphConv->LN/BN->ReLU')\n","        else:\n","            raise Exception('Unknown block Type')\n","\n","        self.gcns = torch.nn.ModuleList()\n","        self.norms = torch.nn.ModuleList()\n","\n","        if self.add_virtual_node:\n","            self.virtualnode_embedding = torch.nn.Embedding(1, hidden_channels)\n","            torch.nn.init.constant_(self.virtualnode_embedding.weight.data, 0)\n","\n","            self.mlp_virtualnode_list = torch.nn.ModuleList()\n","\n","            for layer in range(self.num_layers - 1):\n","                self.mlp_virtualnode_list.append(MLP([hidden_channels]*3,\n","                                                     norm=norm))\n","\n","        for layer in range(self.num_layers):\n","            if conv == 'gen':\n","                gcn = GENConv(hidden_channels, hidden_channels,\n","                              aggr=aggr,\n","                              t=t, learn_t=self.learn_t,\n","                              p=p, learn_p=self.learn_p,\n","                              y=y, learn_y=self.learn_p,\n","                              msg_norm=self.msg_norm, learn_msg_scale=learn_msg_scale,\n","                              encode_edge=self.conv_encode_edge, bond_encoder=True,\n","                              norm=norm, mlp_layers=mlp_layers)\n","            else:\n","                raise Exception('Unknown Conv Type')\n","            self.gcns.append(gcn)\n","            self.norms.append(norm_layer(norm, hidden_channels))\n","\n","        self.atom_encoder = AtomEncoder(emb_dim=hidden_channels)\n","\n","        if not self.conv_encode_edge:\n","            self.bond_encoder = BondEncoder(emb_dim=hidden_channels)\n","\n","        if graph_pooling == \"sum\":\n","            self.pool = global_add_pool\n","        elif graph_pooling == \"mean\":\n","            self.pool = global_mean_pool\n","        elif graph_pooling == \"max\":\n","            self.pool = global_max_pool\n","        else:\n","            raise Exception('Unknown Pool Type')\n","\n","        self.graph_pred_linear = torch.nn.Linear(hidden_channels, num_tasks)\n","\n","    def forward(self, input_batch):\n","\n","        x = input_batch.x\n","        edge_index = input_batch.edge_index\n","        edge_attr = input_batch.edge_attr\n","        batch = input_batch.batch\n","\n","        h = self.atom_encoder(x)\n","\n","        if self.add_virtual_node:\n","            virtualnode_embedding = self.virtualnode_embedding(\n","                torch.zeros(batch[-1].item() + 1).to(edge_index.dtype).to(edge_index.device))\n","            h = h + virtualnode_embedding[batch]\n","\n","        if self.conv_encode_edge:\n","            edge_emb = edge_attr\n","        else:\n","            edge_emb = self.bond_encoder(edge_attr)\n","\n","        if self.block == 'res+':\n","\n","            h = self.gcns[0](h, edge_index, edge_emb)\n","\n","            for layer in range(1, self.num_layers):\n","                h1 = self.norms[layer - 1](h)\n","                h2 = self.activation_func(h1)\n","                h2 = F.dropout(h2, p=self.dropout, training=self.training)\n","\n","                if self.add_virtual_node:\n","                    virtualnode_embedding_temp = global_add_pool(h2, batch) + virtualnode_embedding\n","                    virtualnode_embedding = F.dropout(\n","                        self.mlp_virtualnode_list[layer-1](virtualnode_embedding_temp),\n","                        self.dropout, training=self.training)\n","\n","                    h2 = h2 + virtualnode_embedding[batch]\n","\n","                h = self.gcns[layer](h2, edge_index, edge_emb) + h\n","\n","            h = self.norms[self.num_layers - 1](h)\n","            h = F.dropout(h, p=self.dropout, training=self.training)\n","\n","        elif self.block == 'res':\n","\n","            h = self.activation_func(self.norms[0](self.gcns[0](h, edge_index, edge_emb)))\n","            h = F.dropout(h, p=self.dropout, training=self.training)\n","\n","            for layer in range(1, self.num_layers):\n","                h1 = self.gcns[layer](h, edge_index, edge_emb)\n","                h2 = self.norms[layer](h1)\n","                h = self.activation_func(h2) + h\n","                h = F.dropout(h, p=self.dropout, training=self.training)\n","\n","        elif self.block == 'dense':\n","            raise NotImplementedError('To be implemented')\n","\n","        elif self.block == 'plain':\n","\n","            h = self.activation_func(self.norms[0](self.gcns[0](h, edge_index, edge_emb)))\n","            h = F.dropout(h, p=self.dropout, training=self.training)\n","\n","            for layer in range(1, self.num_layers):\n","                h1 = self.gcns[layer](h, edge_index, edge_emb)\n","                h2 = self.norms[layer](h1)\n","                if layer != (self.num_layers - 1):\n","                    h = self.activation_func(h2)\n","                else:\n","                    h = h2\n","                h = F.dropout(h, p=self.dropout, training=self.training)\n","        else:\n","            raise Exception('Unknown block Type')\n","\n","        h_graph = self.pool(h, batch) # N, 256\n","        #print (h_graph.shape)\n","        #h_graph= self.dropout_fc(h_graph)\n","        return self.graph_pred_linear(h_graph)"],"metadata":{"id":"pN_vXWLOS2Ie","executionInfo":{"status":"ok","timestamp":1745586322215,"user_tz":-120,"elapsed":9,"user":{"displayName":"Thomas G","userId":"10343934438851165110"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["device = torch.device(\"cuda\")\n","model = DeeperGCN(cfg).to(device)\n","criterion = AUCMLoss()\n","optimizer = PESG(model.parameters(), loss_fn=criterion,\n","                 a=criterion.a, b=criterion.b, alpha=criterion.alpha,\n","                 lr=cfg.lr, gamma=cfg.gamma, margin=cfg.margin,\n","                 weight_decay=cfg.weight_decay)\n","\n","def train_epoch():\n","    model.train(); losses=[]\n","    for batch in loaders[\"train\"]:\n","        batch = batch.to(device)\n","        optimizer.zero_grad()\n","        pred = model(batch)\n","        pos = batch.y[:, 0:1]  #\n","        #pos = batch.y.view(-1,1).to(torch.float32)\n","        loss = criterion(pred, pos).to(torch.float32)\n","        loss.backward(); optimizer.step()\n","        losses.append(loss.item())\n","    return np.mean(losses)\n","\n","@torch.no_grad()\n","def eval_split(phase):\n","    model.eval()\n","    ys,ps=[],[]\n","    for batch in loaders[phase]:\n","        batch = batch.to(device)\n","        out = model(batch)\n","        ys.append(batch.y[:, 0:1].cpu().numpy())\n","        ps.append(torch.sigmoid(out).cpu().numpy())\n","    y_true = np.vstack(ys); y_pred = np.vstack(ps)\n","    return Evaluator(cfg.dataset).eval({\"y_true\":y_true,\"y_pred\":y_pred})[\"rocauc\"]\n","\n","best_val,best_ckpt=0,None\n","for epoch in range(1, cfg.epochs_pre+1):\n","    l = train_epoch()\n","    val_auc = eval_split(\"valid\")\n","    if val_auc>best_val:\n","        best_val=val_auc\n","        best_ckpt = f\"pretrained_{epoch}.pth\"\n","        torch.save(model.state_dict(), best_ckpt)\n","    if epoch%50==0:\n","    #if epoch%10==0:\n","\n","        print(f\"Epoch {epoch}: train loss {l:.4f}, val AUC {val_auc:.4f}\")\n","print(\"Best pretrain AUC:\", best_val)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Uj0ocTAWTt2X","executionInfo":{"status":"ok","timestamp":1745588785514,"user_tz":-120,"elapsed":2435518,"user":{"displayName":"Thomas G","userId":"10343934438851165110"}},"outputId":"72add11e-a72b-4174-c74d-850fa557eb2d"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["The number of layers 14 Aggr aggregation method softmax block: res+\n","LN/BN->ReLU->GraphConv->Res\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/libauc/losses/auc.py:111: UserWarning: Input data has no positive sample! Please use 'libauc.sampler.DualSampler' for data resampling!\n","  warnings.warn(\"Input data has no positive sample! Please use 'libauc.sampler.DualSampler' for data resampling!\", UserWarning)\n","/usr/local/lib/python3.11/dist-packages/libauc/losses/auc.py:111: UserWarning: Input data has no positive sample! Please use 'libauc.sampler.DualSampler' for data resampling!\n","  warnings.warn(\"Input data has no positive sample! Please use 'libauc.sampler.DualSampler' for data resampling!\", UserWarning)\n","/usr/local/lib/python3.11/dist-packages/libauc/losses/auc.py:111: UserWarning: Input data has no positive sample! Please use 'libauc.sampler.DualSampler' for data resampling!\n","  warnings.warn(\"Input data has no positive sample! Please use 'libauc.sampler.DualSampler' for data resampling!\", UserWarning)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 50: train loss 0.0254, val AUC 0.7647\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/libauc/losses/auc.py:111: UserWarning: Input data has no positive sample! Please use 'libauc.sampler.DualSampler' for data resampling!\n","  warnings.warn(\"Input data has no positive sample! Please use 'libauc.sampler.DualSampler' for data resampling!\", UserWarning)\n","/usr/local/lib/python3.11/dist-packages/libauc/losses/auc.py:111: UserWarning: Input data has no positive sample! Please use 'libauc.sampler.DualSampler' for data resampling!\n","  warnings.warn(\"Input data has no positive sample! Please use 'libauc.sampler.DualSampler' for data resampling!\", UserWarning)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 100: train loss 0.0224, val AUC 0.7741\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/libauc/losses/auc.py:111: UserWarning: Input data has no positive sample! Please use 'libauc.sampler.DualSampler' for data resampling!\n","  warnings.warn(\"Input data has no positive sample! Please use 'libauc.sampler.DualSampler' for data resampling!\", UserWarning)\n","/usr/local/lib/python3.11/dist-packages/libauc/losses/auc.py:111: UserWarning: Input data has no positive sample! Please use 'libauc.sampler.DualSampler' for data resampling!\n","  warnings.warn(\"Input data has no positive sample! Please use 'libauc.sampler.DualSampler' for data resampling!\", UserWarning)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 150: train loss 0.0214, val AUC 0.7782\n","Epoch 200: train loss 0.0212, val AUC 0.7827\n","Epoch 250: train loss 0.0199, val AUC 0.7760\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/libauc/losses/auc.py:111: UserWarning: Input data has no positive sample! Please use 'libauc.sampler.DualSampler' for data resampling!\n","  warnings.warn(\"Input data has no positive sample! Please use 'libauc.sampler.DualSampler' for data resampling!\", UserWarning)\n","/usr/local/lib/python3.11/dist-packages/libauc/losses/auc.py:111: UserWarning: Input data has no positive sample! Please use 'libauc.sampler.DualSampler' for data resampling!\n","  warnings.warn(\"Input data has no positive sample! Please use 'libauc.sampler.DualSampler' for data resampling!\", UserWarning)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 300: train loss 0.0194, val AUC 0.7918\n","Best pretrain AUC: 0.792628110915148\n"]}]},{"cell_type":"code","source":["class DeeperGCNAtt(torch.nn.Module):\n","    def __init__(self, args):\n","        super(DeeperGCNAtt, self).__init__()\n","\n","        self.num_layers = args.num_layers\n","        self.dropout = args.dropout\n","        self.block = args.block\n","        self.conv_encode_edge = args.conv_encode_edge\n","        self.add_virtual_node = args.add_virtual_node\n","\n","        hidden_channels = args.hidden_channels\n","        num_tasks = args.num_tasks\n","        conv = args.conv\n","        aggr = args.gcn_aggr\n","        t = args.t\n","        self.learn_t = args.learn_t\n","        p = args.p\n","        self.learn_p = args.learn_p\n","        y = args.y\n","        self.learn_y = args.learn_y\n","\n","        self.beta = torch.nn.Parameter(torch.Tensor([0.5]), requires_grad=True)\n","\n","        self.msg_norm = args.msg_norm\n","        learn_msg_scale = args.learn_msg_scale\n","        self.activation_func = F.relu if args.activations=='relu' else F.elu\n","\n","        norm = args.norm\n","        mlp_layers = args.mlp_layers\n","\n","        graph_pooling = args.graph_pooling\n","\n","        print('The number of layers {}'.format(self.num_layers),\n","              'Aggr aggregation method {}'.format(aggr),\n","              'block: {}'.format(self.block))\n","        if self.block == 'res+':\n","            print('LN/BN->ReLU->GraphConv->Res')\n","        elif self.block == 'res':\n","            print('GraphConv->LN/BN->ReLU->Res')\n","        elif self.block == 'dense':\n","            raise NotImplementedError('To be implemented')\n","        elif self.block == \"plain\":\n","            print('GraphConv->LN/BN->ReLU')\n","        else:\n","            raise Exception('Unknown block Type')\n","\n","        self.gcns = torch.nn.ModuleList()\n","        self.norms = torch.nn.ModuleList()\n","\n","        if self.add_virtual_node:\n","            self.virtualnode_embedding = torch.nn.Embedding(1, hidden_channels)\n","            torch.nn.init.constant_(self.virtualnode_embedding.weight.data, 0)\n","\n","            self.mlp_virtualnode_list = torch.nn.ModuleList()\n","\n","            for layer in range(self.num_layers - 1):\n","                self.mlp_virtualnode_list.append(MLP([hidden_channels]*3,\n","                                                     norm=norm))\n","\n","        for layer in range(self.num_layers):\n","            if conv == 'gen':\n","                gcn = GENConv(hidden_channels, hidden_channels,\n","                              aggr=aggr,\n","                              t=t, learn_t=self.learn_t,\n","                              p=p, learn_p=self.learn_p,\n","                              y=y, learn_y=self.learn_p,\n","                              msg_norm=self.msg_norm, learn_msg_scale=learn_msg_scale,\n","                              encode_edge=self.conv_encode_edge, bond_encoder=True,\n","                              norm=norm, mlp_layers=mlp_layers)\n","            else:\n","                raise Exception('Unknown Conv Type')\n","            self.gcns.append(gcn)\n","            self.norms.append(norm_layer(norm, hidden_channels))\n","\n","        self.atom_encoder = AtomEncoder(emb_dim=hidden_channels)\n","\n","        if not self.conv_encode_edge:\n","            self.bond_encoder = BondEncoder(emb_dim=hidden_channels)\n","\n","        if graph_pooling == \"sum\":\n","            self.pool = global_add_pool\n","        elif graph_pooling == \"mean\":\n","            self.pool = global_mean_pool\n","        elif graph_pooling == \"max\":\n","            self.pool = global_max_pool\n","        else:\n","            raise Exception('Unknown Pool Type')\n","\n","        self.graph_pred_linear = torch.nn.Linear(hidden_channels, num_tasks)\n","\n","    def forward(self, input_batch, mode='train'):\n","        x = input_batch.x\n","\n","        edge_index = input_batch.edge_index\n","        edge_attr = input_batch.edge_attr\n","        batch = input_batch.batch\n","\n","        h = self.atom_encoder(x)\n","\n","        if self.add_virtual_node:\n","            virtualnode_embedding = self.virtualnode_embedding(\n","                torch.zeros(batch[-1].item() + 1).to(edge_index.dtype).to(edge_index.device))\n","            h = h + virtualnode_embedding[batch]\n","\n","        if self.conv_encode_edge:\n","            edge_emb = edge_attr\n","        else:\n","            edge_emb = self.bond_encoder(edge_attr)\n","\n","        if self.block == 'res+':\n","\n","            h = self.gcns[0](h, edge_index, edge_emb)\n","\n","            for layer in range(1, self.num_layers):\n","                h1 = self.norms[layer - 1](h)\n","                h2 = self.activation_func(h1)\n","                h2 = F.dropout(h2, p=self.dropout, training=self.training)\n","\n","                if self.add_virtual_node:\n","                    virtualnode_embedding_temp = global_add_pool(h2, batch) + virtualnode_embedding\n","                    virtualnode_embedding = F.dropout(\n","                        self.mlp_virtualnode_list[layer-1](virtualnode_embedding_temp),\n","                        self.dropout, training=self.training)\n","\n","                    h2 = h2 + virtualnode_embedding[batch]\n","\n","                h = self.gcns[layer](h2, edge_index, edge_emb) + h\n","\n","            h = self.norms[self.num_layers - 1](h)\n","            h = F.dropout(h, p=self.dropout, training=self.training)\n","\n","        elif self.block == 'res':\n","\n","            h = self.activation_func(self.norms[0](self.gcns[0](h, edge_index, edge_emb)))\n","            h = F.dropout(h, p=self.dropout, training=self.training)\n","\n","            for layer in range(1, self.num_layers):\n","                h1 = self.gcns[layer](h, edge_index, edge_emb)\n","                h2 = self.norms[layer](h1)\n","                h = self.activation_func(h2) + h\n","                h = F.dropout(h, p=self.dropout, training=self.training)\n","\n","        elif self.block == 'dense':\n","            raise NotImplementedError('To be implemented')\n","\n","        elif self.block == 'plain':\n","\n","            h = self.activation_func(self.norms[0](self.gcns[0](h, edge_index, edge_emb)))\n","            h = F.dropout(h, p=self.dropout, training=self.training)\n","\n","            for layer in range(1, self.num_layers):\n","                h1 = self.gcns[layer](h, edge_index, edge_emb)\n","                h2 = self.norms[layer](h1)\n","                if layer != (self.num_layers - 1):\n","                    h = self.activation_func(h2)\n","                else:\n","                    h = h2\n","                h = F.dropout(h, p=self.dropout, training=self.training)\n","        else:\n","            raise Exception('Unknown block Type')\n","\n","        h_graph = self.pool(h, batch) # N, 256\n","\n","        dcn_pred = self.graph_pred_linear(h_graph)\n","        rf_pred = input_batch.y[:, 1]\n","        return (1-self.beta)*torch.sigmoid(dcn_pred).reshape(-1, 1) + (self.beta) * rf_pred.reshape(-1,1)"],"metadata":{"id":"3wOl0ja7-vd2","executionInfo":{"status":"ok","timestamp":1745588785555,"user_tz":-120,"elapsed":9,"user":{"displayName":"Thomas G","userId":"10343934438851165110"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["finetune_model = DeeperGCNAtt(cfg).to(device)\n","finetune_model.load_state_dict(torch.load(best_ckpt,map_location = device), strict=False)\n","\n","optimizer_ft = PESG(\n","    finetune_model.parameters(), loss_fn=criterion,\n","    a=criterion.a, b=criterion.b, alpha=criterion.alpha,\n","    lr=cfg.lr * 0.1, gamma=cfg.gamma, margin=cfg.margin,\n","    weight_decay=cfg.weight_decay\n",")\n","\n","def train_epoch_ft():\n","    finetune_model.train()\n","    losses = []\n","    for batch in loaders[\"train\"]:\n","        batch = batch.to(device)\n","        optimizer_ft.zero_grad()\n","\n","        # 1) Forward pass: model automatically reads both true & rf from batch.y\n","        pred = finetune_model(batch, mode=\"train\")  # (B,1)\n","\n","        # 2) Extract only the true labels for computing the loss\n","        true = batch.y[:, 0:1].to(torch.float32)     # (B,1)\n","\n","        # 3) Compute AUC-Margin loss on [true vs. fused pred]\n","        loss = criterion(pred, true)\n","        loss.backward()\n","        optimizer_ft.step()\n","        losses.append(loss.item())\n","\n","    return float(np.mean(losses))\n","\n","\n","@torch.no_grad()\n","def eval_split_ft(phase):\n","    finetune_model.eval()\n","    ys, ps = [], []\n","    for batch in loaders[phase]:\n","        batch = batch.to(device)\n","\n","        # model(batch, mode=\"test\") again fuses GNN+RF\n","        out = finetune_model(batch, mode=\"test\")    # (B,1)\n","\n","        # collect true labels\n","        ys.append(batch.y[:, 0:1].cpu().numpy())    # (B,1)\n","\n","        # collect fused predictions\n","        ps.append(out.cpu().numpy())                # (B,1)\n","\n","    y_true = np.vstack(ys)                         # (N_batch,1)\n","    y_pred = np.vstack(ps)                         # (N_batch,1)\n","    return Evaluator(cfg.dataset).eval({\n","        \"y_true\": y_true,\n","        \"y_pred\": y_pred\n","    })[\"rocauc\"]\n","\n","\n","# def train_epoch_ft():\n","#     finetune_model.train()\n","#     losses = []\n","#     for batch in loaders[\"train\"]:\n","#         batch = batch.to(device)\n","#         optimizer_ft.zero_grad()\n","#         pred = finetune_model(batch, mode=\"train\")     # now uses pretrained backbone\n","#         true = batch.y[:, 0:1].float()\n","#         loss = criterion(pred, true)\n","#         loss.backward()\n","#         optimizer_ft.step()\n","#         losses.append(loss.item())\n","#     return float(np.mean(losses))\n","\n","# @torch.no_grad()\n","# def eval_split_ft(phase):\n","#     finetune_model.eval()\n","#     ys, ps = [], []\n","#     for batch in loaders[phase]:\n","#         batch = batch.to(device)\n","#         out = finetune_model(batch, mode=\"test\")          # (B,1)\n","#         ys.append(batch.y[:, 0:1].cpu().numpy())          # (B,1)\n","#         ps.append(out.cpu().numpy())                      # (B,1)\n","#     y_true = np.vstack(ys)\n","#     y_pred = np.vstack(ps)\n","#     return Evaluator(cfg.dataset).eval({\"y_true\": y_true, \"y_pred\": y_pred})[\"rocauc\"]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lFEbhLlA--Gl","executionInfo":{"status":"ok","timestamp":1745588785599,"user_tz":-120,"elapsed":40,"user":{"displayName":"Thomas G","userId":"10343934438851165110"}},"outputId":"bbb1d3a1-6b39-4b0d-d82d-a390a51f7009"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["The number of layers 14 Aggr aggregation method softmax block: res+\n","LN/BN->ReLU->GraphConv->Res\n"]}]},{"cell_type":"code","source":["best_val2, best_ckpt2 = 0.0, None\n","\n","for epoch in range(1, cfg.epochs_ft + 1):\n","    # 1) One epoch of finetune training\n","    train_loss = train_epoch_ft()\n","\n","    # 2) Evaluate on the held-out valid split\n","    val_auc = eval_split_ft(\"valid\")\n","\n","    # 3) If it’s the best so far, save the finetuned weights\n","    if val_auc > best_val2:\n","        best_val2   = val_auc\n","        best_ckpt2  = f\"finetuned_epoch{epoch}.pth\"\n","        torch.save(finetune_model.state_dict(), best_ckpt2)\n","\n","    # 4) Log every 20 epochs\n","    if epoch % 20 == 0:\n","        print(f\"[FT] Epoch {epoch:3d}: train loss {train_loss:.4f}, valid AUC {val_auc:.4f}\")\n","\n","# Final report\n","print(f\"✅ Best finetune valid AUC: {best_val2:.4f} → {best_ckpt2}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hCyPa8YABLNq","executionInfo":{"status":"ok","timestamp":1745589617426,"user_tz":-120,"elapsed":831825,"user":{"displayName":"Thomas G","userId":"10343934438851165110"}},"outputId":"abbe2528-9f87-412b-8e27-12c7ecb4dce1"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["[FT] Epoch  20: train loss 0.0170, valid AUC 0.8161\n","[FT] Epoch  40: train loss 0.0128, valid AUC 0.8162\n","[FT] Epoch  60: train loss 0.0097, valid AUC 0.8149\n","[FT] Epoch  80: train loss 0.0075, valid AUC 0.8143\n","[FT] Epoch 100: train loss 0.0059, valid AUC 0.8146\n","✅ Best finetune valid AUC: 0.8173 → finetuned_epoch17.pth\n"]}]},{"cell_type":"code","source":["finetune_model.load_state_dict(torch.load(best_ckpt2), strict=False)\n","finetune_model.to(device)\n","\n","# 3) Now evaluate on the test split\n","test_auc = eval_split_ft(\"test\")\n","print(\"✔️ Test ROC-AUC of best finetuned model:\", test_auc)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"feYDhRhFCQYG","executionInfo":{"status":"ok","timestamp":1745589618431,"user_tz":-120,"elapsed":1017,"user":{"displayName":"Thomas G","userId":"10343934438851165110"}},"outputId":"0315d9a1-9ab2-4be7-b993-a1b7d04467e2"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["✔️ Test ROC-AUC of best finetuned model: 0.8075822244539291\n"]}]}]}